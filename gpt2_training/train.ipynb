{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load ROCStories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/h/brandon/.local/lib/python3.9/site-packages/transformers/data/datasets/language_modeling.py:54: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the input data\n",
    "train_path = '/h/brandon/internship/Uncover_implicit_bias/gpt2_training/data/ROCStories_winter2017.csv'  # This file should contain one story per line\n",
    "val_path = 'path_to_validation_data.txt'  # This file should contain one story per line\n",
    "\n",
    "# Create a dataset and collator\n",
    "train_dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=train_path,\n",
    "    block_size=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some samples from the training set:\n",
      "\n",
      "Sample 1:\n",
      "storyid,storytitle,sentence1,sentence2,sentence3,sentence4,sentence5\n",
      "8bbe6d11-1e2e-413c-bf81-eaea05f4f1bd,David Drops the Weight,David noticed he had put on a lot of weight recently.,He examined his habits to try and figure out the reason.,He realized he'd been eating too much fast food lately.,He stopped going to burger places and started a vegetarian diet.,\"After a few weeks, he started to feel much better.\"\n",
      "0beabab2-fb49-\n",
      "\n",
      "Sample 2:\n",
      "460e-a6e6-f35a202e3348,Frustration,Tom had a very short temper.,One day a guest made him very angry.,He punched a hole in the wall of his house.,Tom's guest became afraid and left quickly.,Tom sat on his couch filled with regret about his actions.\n",
      "87da1a22-df0b-410c-b186-439700b70ba6,Marcus Buys Khakis,Marcus needed clothing for a business casual event.,All of his clothes were either too formal or too casual.,He decided to buy a pair of khakis.,\n",
      "\n",
      "Sample 3:\n",
      "The pair he bought fit him perfectly.,Marcus was happy to have the right clothes for the event.\n",
      "2d16bcd6-692a-4fc0-8e7c-4a6f81d9efa9,Different Opinions,Bobby thought Bill should buy a trailer and haul it with his car.,Bill thought a truck would be better for what he needed.,Bobby pointed out two vehicles were much more expensive.,Bill was set in his ways with conventional thinking.,He ended up buying the truck he wanted despite Bobby's advice.\n",
      "c71bb23b-7731-4\n",
      "\n",
      "Sample 4:\n",
      "233-8298-76ba6886cee1,Overcoming shortcomings,John was a pastor with a very bad memory.,He tried to memorize his sermons many days in advance but to no avail.,He decided to learn to sing to overcome his handicap.,He then made all his sermons into music and sang them on Sundays.,His congregation was delighted and so was he.\n",
      "4d7b022e-25d2-4300-a9b0-24ab35f4045b,Melody's trip to the aquarium.,Melody's parents surprised her with a trip to the big\n",
      "\n",
      "Sample 5:\n",
      " aquarium.,Melody took a nap during the two hour car ride to the aquarium.,\"When they arrived, Melody was energetic and excited.\",\"At the aquarium Melody saw sharks, tropical fish and many others.\",\"After five hours at the aquarium, Melody and her family drove home.\"\n",
      "8036c905-f23e-4976-83a1-85d679b5e0c2,Pop Quiz,The math teacher announced a pop quiz as class began.,\"While some students complained, he began passing out the quiz.\",I took out my pencil and began to work.,\"About 5 minutes later, I finished.\",I stood\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to decode the token IDs\n",
    "def decode_sample(token_ids):\n",
    "    return tokenizer.decode(token_ids, clean_up_tokenization_spaces=True, skip_special_tokens=True)\n",
    "\n",
    "# Visualize the first few samples from train_dataset\n",
    "print(\"Some samples from the training set:\\n\")\n",
    "for i in range(5):  # let's print out the first 5 samples\n",
    "    token_ids = train_dataset[i]  # get the token IDs for the i-th sample\n",
    "    text = decode_sample(token_ids)  # decode the token IDs to text\n",
    "    print(f\"Sample {i + 1}:\\n{text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False\n",
    ")\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',         # Output directory\n",
    "    overwrite_output_dir=True,      # Overwrite the content of the output directory\n",
    "    num_train_epochs=3,             # Number of training epochs\n",
    "    per_device_train_batch_size=4,  # Batch size for training\n",
    "    per_device_eval_batch_size=4,   # Batch size for evaluation\n",
    "    eval_steps=400,                 # Evaluation step\n",
    "    save_steps=800,                 # After # steps model is saved\n",
    "    warmup_steps=500,               # Warmup steps\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "trainer.save_model(\"fine_tuned_gpt2_rocstories\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "implicit_biases2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
